
# Transformer

This repository contains a transformer model built from scratch in PyTorch, with an in-depth explanation of each layer. Using a simple example sentence, we detail each computational step, matrix transformation, and operation within the model.

## Table of Contents

1. [Introduction](#introduction)
2. [Running Example: "I like cats"](#running-example)
3. [Components & Detailed Calculations](#components-and-detailed-calculations)
    - [Token Embedding](#token-embedding)
    - [Positional Encoding](#positional-encoding)
    - [Self-Attention](#self-attention)
    - [Feed-Forward Network](#feed-forward-network)
    - [Transformer Block](#transformer-block)
    - [Output Layer](#output-layer)

## Introduction

This project explores each part of a transformer model, inspired by "Attention Is All You Need,". By using the sentence **"I like cats,"** we calculate and display each transformation matrix, illustrating how the transformer processes input tokens.

## Running Example: "I like cats"

Weâ€™ll follow **"I like cats"** step-by-step, with detailed matrix manipulations and calculations.

### Example Details

1. **Input Sentence**: "I like cats"
2. **Token IDs**: Assign each word an ID (e.g., `{ "I": 1, "like": 2, "cats": 3 }`).
3. **Embedding Dimension**: For simplicity, we set $d_{\text{model}} = 3$, with a vocabulary size $ V = 10 $.

Each transformation matrix will be explicitly calculated and explained.

## Components and Detailed Calculations

### Token Embedding

Tokens are converted to embeddings using an embedding matrix $ E $ with dimensions $ V \times d_{\text{model}} = 10 \times 3 $. Each word maps to a 3-dimensional vector.

- **Embedding Matrix $ E $** (simplified for explanation):
  $
  E = \begin{bmatrix}
    0.1 & 0.2 & 0.3 \\ 
    0.4 & 0.5 & 0.6 \\ 
    0.7 & 0.8 & 0.9 \\ 
    \vdots & \vdots & \vdots 
  \end{bmatrix}
  $

- **Token Embeddings for "I," "like," "cats"** (example calculation):
  - "I" (ID 1): $ E[1] = [0.4, 0.5, 0.6] $
  - "like" (ID 2): $ E[2] = [0.7, 0.8, 0.9] $
  - "cats" (ID 3): $ E[3] = [0.1, 0.2, 0.3] $

The token embedding layer outputs a matrix:
$ X_{\text{emb}} = \begin{bmatrix} 
  0.4 & 0.5 & 0.6 \\
  0.7 & 0.8 & 0.9 \\
  0.1 & 0.2 & 0.3 
\end{bmatrix}
$

### Positional Encoding

In this implementation, positional encodings are learned embeddings, allowing the model to discover optimal position representations through training. Each position in the input sequence has a corresponding embedding in a positional embedding table, with dimensions $ \text{block size} \times d_{\text{model}} $.

- **Positional Embedding Matrix**: 
  $
  P = \begin{bmatrix} 
      p_{1,1} & p_{1,2} & p_{1,3} \\ 
      p_{2,1} & p_{2,2} & p_{2,3} \\ 
      \vdots & \vdots & \vdots \\
      p_{T,1} & p_{T,2} & p_{T,3}
  \end{bmatrix}
  $
  where $ T $ is the sequence length (block size).

The positional embeddings are added element-wise to the token embeddings, giving each position a unique encoding:

$X_{\text{emb}} + P = \begin{bmatrix} 
  0.4 + p_{1,1} & 0.5 + p_{1,2} & 0.6 + p_{1,3} \\ 
  0.7 + p_{2,1} & 0.8 + p_{2,2} & 0.9 + p_{2,3} \\ 
  0.1 + p_{3,1} & 0.2 + p_{3,2} & 0.3 + p_{3,3} 
\end{bmatrix}
$

We add $ \text{PE} $ to $ X_{\text{emb}} $ for a combined embedding.

### Self-Attention

#### Step 1: Calculate $ Q, K, V $ Matrices

We calculate query $ Q $, key $ K $, and value $ V $ matrices by multiplying $ X $ with learned weights $ W^Q $, $ W^K $, and $ W^V $, each sized $ d_{\text{model}} \times d_k $ (assume $ d_k = d_{\text{model}} $ for simplicity).

Example:
$Q = X W^Q, \quad K = X W^K, \quad V = X W^V$

#### Step 2: Scaled Dot-Product Attention

Attention scores are derived by calculating $ QK^T $ and scaling by $ \frac{1}{\sqrt{d_k}} $:
$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$

This step will show matrices for $ Q $, $ K $, and $ V $, intermediate products $ QK^T $, scaling, softmax, and the weighted sum result.

### Feed-Forward Network

The feed-forward network applies two linear transformations with a ReLU activation in between. For each token representation, we calculate:
1. **Linear 1**: $ X_{\text{ffn}} = \text{ReLU}(X W_1 + b_1) $
2. **Linear 2**: $ \text{output} = X_{\text{ffn}} W_2 + b_2 $

Example weight matrices and their multiplication with attention outputs will be detailed here.

### Transformer Block

A transformer block is a sequence of:
1. **Self-Attention** (computed above)
2. **Feed-Forward Network** (computed above)
3. **Residual Connections**: Adding the inputs back after attention and feed-forward steps
4. **Layer Normalization**: Ensuring normalized outputs

Each residual and normalization step will include calculations with intermediate matrix outputs.

### Output Layer

The output layer projects the processed embeddings back to the vocabulary size $ V $ with a linear transformation:
$\text{logits} = X W_{\text{out}} + b_{\text{out}}
$
where $ W_{\text{out}} $ has dimensions $ d_{\text{model}} \times V $.