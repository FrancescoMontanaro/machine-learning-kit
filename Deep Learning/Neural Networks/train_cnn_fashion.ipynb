{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf # using tensorflow only for the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import the module from the custom library\n",
    "from src import utils\n",
    "from src import layers\n",
    "from src import metrics\n",
    "from src import callbacks\n",
    "from src import optimizers\n",
    "from src import activations\n",
    "from src import loss_functions\n",
    "from src import FeedForward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_pct = 0.2 # Percentage of samples to use for testing\n",
    "train_valid_split = 0.1 # Percentage of samples to use for validation\n",
    "learning_rate = 1e-4 # Learning rate for the optimizer\n",
    "batch_size = 32 # Number of samples to use for each batch\n",
    "epochs = 50 # Number of epochs to train the model\n",
    "seed = 1234 # Seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() # type: ignore\n",
    "\n",
    "# Add a channel dimension to the images\n",
    "X_train = np.expand_dims(X_train, axis=-1) # type: ignore\n",
    "X_test = np.expand_dims(X_test, axis=-1) # type: ignore\n",
    "\n",
    "# Extract the number of classes in the dataset\n",
    "num_classes = len(np.unique(y_train)) # type: ignore\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=train_valid_split, random_state=seed) # type: ignore\n",
    "\n",
    "# Print the dataset information\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print('Training set:', X_train.shape, y_train.shape)\n",
    "print('Validation set:', X_valid.shape, y_valid.shape)\n",
    "print('Testing set:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize the input data by dividing by the maximum value in the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    - X (np.ndarray): The input data to normalize\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: The normalized input data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize the input data\n",
    "    return X / 255.0\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = normalize(X_train)\n",
    "X_valid = normalize(X_valid)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target values to one-hot encoded vectors\n",
    "y_train_encoded = utils.one_hot_encoding(y=y_train, n_classes=num_classes)\n",
    "y_valid_encoded = utils.one_hot_encoding(y=y_valid, n_classes=num_classes)\n",
    "\n",
    "# Print one sample encoding\n",
    "print(\"Sample target value:\", y_train[0])\n",
    "print(\"One-hot encoded value:\", y_valid_encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples: list[np.ndarray], labels: list[np.ndarray]) -> None:\n",
    "    \"\"\"\n",
    "    Plot the samples in a grid.\n",
    "    \n",
    "    Parameters:\n",
    "    - samples (list[np.ndarray]): The samples to plot\n",
    "    - labels (list[np.ndarray]): The labels of the samples\n",
    "    \"\"\"\n",
    "        \n",
    "    # Plot the samples in a grid\n",
    "    fig, axes = plt.subplots(1, len(samples), figsize=(20, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(samples[i], cmap='gray')\n",
    "        ax.set_title(f'Label: {labels[i]}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "# Plot the first 10 samples\n",
    "plot_samples(list(X_train[:10]), y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = FeedForward([\n",
    "    layers.Conv2D(num_filters=32, kernel_size=(3, 3), activation=activations.ReLU(), padding='same'),\n",
    "    layers.MaxPool2D(size=(2, 2), stride=(2, 2)),\n",
    "    layers.Dropout(rate=0.2),\n",
    "    layers.Conv2D(num_filters=64, kernel_size=(3, 3), activation=activations.ReLU(), padding='same'),\n",
    "    layers.MaxPool2D(size=(2, 2), stride=(2, 2)),\n",
    "    layers.Dropout(rate=0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.BatchNormalization(momentum=0.9),\n",
    "    layers.Dense(num_units=128, activation=activations.ReLU()),\n",
    "    layers.Dense(num_units=num_classes, activation=activations.Softmax())\n",
    "])\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = loss_functions.CrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "model(X_train[:batch_size]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train = X_train,\n",
    "    y_train = y_train_encoded,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    X_valid = X_valid,\n",
    "    y_valid = y_valid_encoded,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    metrics = [metrics.accuracy],\n",
    "    callbacks = [callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "utils.plot_history(history[\"loss\"], history[\"val_loss\"], \"Training and Validation Loss\", \"Epoch\", \"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels\n",
    "predictions = model(X_test)\n",
    "\n",
    "# Apply the argmax function to the predictions\n",
    "predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy\n",
    "accuracy = metrics.accuracy(predictions, y_test)\n",
    "confusion_matrix = metrics.confusion_matrix(num_classes, predictions, y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "utils.plot_confusion_matrix(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
